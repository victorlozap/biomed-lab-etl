# üè• Documentaci√≥n T√©cnica: Pipeline ETL Biom√©dico

## 1. Contexto del Negocio
**Problema:** Un laboratorio cl√≠nico con 3 sedes descentralizadas genera reportes diarios en formatos no estandarizados (CSV, Excel, JSON). Esto impide la consolidaci√≥n de informaci√≥n y retrasa la toma de decisiones cr√≠ticas.

**Objetivo:** Construir un Data Warehouse centralizado que permita la ingesta autom√°tica, limpieza y an√°lisis de estos datos en tiempo cercano al real.

---

## 2. Decisiones de Arquitectura

### üèóÔ∏è Contenedorizaci√≥n (Docker)
* **Decisi√≥n:** Se utiliza Docker para desplegar la base de datos PostgreSQL.
* **Justificaci√≥n:** Garantiza que el entorno sea reproducible. Cualquier ingeniero puede clonar el repositorio y levantar la infraestructura con un solo comando (`docker-compose up`), sin lidiar con instalaciones locales o conflictos de versiones.

### üóÑÔ∏è Motor de Base de Datos (PostgreSQL)
* **Decisi√≥n:** PostgreSQL 13.
* **Justificaci√≥n:** Elegido por su robustez en tipos de datos y conformidad con est√°ndares SQL. Su capacidad nativa para manejar JSONB lo hace ideal para futuros datos m√©dicos semi-estructurados (como FHIR).

### üêç Lenguaje de Orquestaci√≥n (Python)
* **Decisi√≥n:** Python 3.9 + Pandas + SQLAlchemy.
* **Justificaci√≥n:** Pandas ofrece la mayor flexibilidad para manipulaci√≥n de dataframes y limpieza de datos sucios. SQLAlchemy abstrae la conexi√≥n a base de datos, previniendo inyecci√≥n SQL y facilitando el mantenimiento.

---

## 3. Estrategia ETL (Extract, Transform, Load)

### Fase 1: Extracci√≥n
Se desarrollaron conectores espec√≠ficos para cada fuente:
1. **Sede Central:** Archivos CSV estructurados.
2. **Sede Norte:** Archivos Excel (.xlsx) con encabezados en ingl√©s.
3. **Sede Sur:** Archivos JSON anidados y con problemas de calidad (texto sucio).

### Fase 2: Transformaci√≥n (Limpieza)
El pipeline aplica las siguientes reglas de negocio:
* **Normalizaci√≥n de Encabezados:** Todos los campos se renombran al espa√±ol (`nivel_glucosa`, `fecha_muestra`).
* **Limpieza de Tipos:** Se eliminan unidades de texto (ej: "108 mg/dL" -> 108) para permitir operaciones matem√°ticas.
* **Manejo de Nulos:** Se descartan registros sin mediciones v√°lidas.
* **Trazabilidad:** Se inyecta la columna `fuente_sede` para auditar el origen del dato.

### Fase 3: Carga
Carga en modo `replace` (para desarrollo) sobre la tabla `hechos_glucosa` en el esquema p√∫blico de PostgreSQL.

---

## 4. Ejecuci√≥n y Pruebas
El pipeline se ejecuta mediante el script `01_etl_pipeline.py`, procesando exitosamente lotes de datos sint√©ticos generados con la librer√≠a `Faker`.

---

## 5. An√°lisis de Resultados (Data Analytics)

Tras la centralizaci√≥n, se ejecut√≥ la siguiente consulta SQL para evaluar la salud poblacional por sede:

```sql
SELECT fuente_sede, ROUND(AVG(nivel_glucosa), 1) as promedio FROM hechos_glucosa GROUP BY fuente_sede;

### üö® Hallazgo Cr√≠tico
Se detect√≥ una anomal√≠a significativa en la **Sede Sur**, la cual presenta un promedio de glucosa superior a **170 mg/dL**, en contraste con el promedio normal (~100 mg/dL) de las otras sedes. 

**Hip√≥tesis:**
1. Error sistem√°tico en la calibraci√≥n de los equipos de laboratorio de la Sede Sur.
2. Factor de riesgo epidemiol√≥gico en la poblaci√≥n de dicha zona geogr√°fica.

**Siguiente paso:** Se recomienda auditor√≠a t√©cnica inmediata a los equipos de la Sede Sur.